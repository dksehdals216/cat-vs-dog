
Implementatino of Kaggle's Cats vs Dogs dataset.
This project uses a vanilla cnn, and is part of a University Project.

Implemented with tensorflow, while the dense layer uses tflearn to simplify things.


Done:

    Show multiple results instead of a single result.
    Show random results instead of the first 16 that comes out from matplotlib.
    Implemented Batch normalization for Pre-ReLU layers and fully connected layer.
    
Todo:

    Implement a Xavier initialization or any kind of weight initialization
    Compare Batch Normalization results before and after ReLU
    Try different architecture structure, layers, and neurons.
    Try Dropout at all stages or multiple stages of the architecture.
    Try other optimizers besides Adam (RSMdrop)
    
    
Consider:
    
    Early Stopping
    Possibly attempt to fine tune hyperparameters
    Experiment with learning rate (Drops by percentage, or drops at certain epochs)

    
